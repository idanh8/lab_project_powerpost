{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8d5d742-15c9-45e5-803c-7e347593b80f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Step 1. Run the 2 cells below, and enter your data into the widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c77dab61-0a52-4945-ad29-336a75633394",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "pip install -q -U google-generativeai\n",
    "!pip install diffusers\n",
    "%pip install torch\n",
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f591e451-dfe4-4b7b-90aa-22340bf2e85c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create widgets for user input\n",
    "\n",
    "dbutils.widgets.text(\"about\", \"\", \"Please enter the text from the about section of your LinkedIn profile:\")\n",
    "dbutils.widgets.text(\"company_size\", \"0\", \"Please enter the size of your company (number of employees):\")\n",
    "dbutils.widgets.dropdown(\"company_type\", \"Other\", [\"Self-Owned\", \"Privately Held\", 'Partnership', 'Educational', 'Nonprofit', 'Government Agency', 'Public Company', 'Self-Employed', 'Other'], \"Please select the type of company you work in:\")\n",
    "dbutils.widgets.text(\"followers\", \"0\", \"Please enter the number of followers you have:\")\n",
    "dbutils.widgets.dropdown(\"avatar\", \"Profile Picture\", [\"Profile Picture\", \"Stock Image\"], \"Do you have a profile picture or a stock image?:\")\n",
    "\n",
    "\n",
    "dbutils.widgets.text(\"post_text\", \"\", \"Please enter the text of your post, or keywords:\")\n",
    "dbutils.widgets.dropdown(\"post_type\", \"Work\", [\"Work\", \"Education & Self-Improvement\", 'Science & Tech', 'Inspiration', 'General'], \"Please select the type of post you want to create:\")\n",
    "dbutils.widgets.dropdown(\"image\", \"No, please give me an effective prompt that I can use myself\", [\"I am willing to wait 30 minutes for a generated image\", \"No, please give me an effective prompt that I can use myself\"], \"Please indicate your image generation preference:\")\n",
    "dbutils.widgets.text(\"submit\", \"Submit\", \"Type submit and hit enter please\")\n",
    "\n",
    "# Function to be triggered when the button is pressed\n",
    "def on_submit():\n",
    "    # Retrieve the input values\n",
    "    about = dbutils.widgets.get(\"about\")\n",
    "    company_size = int(dbutils.widgets.get(\"company_size\"))\n",
    "    company_type = dbutils.widgets.get(\"company_type\")\n",
    "    followers = int(dbutils.widgets.get(\"followers\"))\n",
    "    avatar_res = dbutils.widgets.get(\"avatar\")\n",
    "    avatar = 1 if avatar_res == 'Profile Picture' else 0\n",
    "    image= dbutils.widgets.get(\"image\")\n",
    "\n",
    "\n",
    "    post_text = dbutils.widgets.get(\"post_text\")\n",
    "    post_type = dbutils.widgets.get(\"post_type\")\n",
    "    \n",
    "    return about, company_size, company_type, followers, avatar, image, post_text, post_type\n",
    "    \n",
    "\n",
    "# Register the function to be called when the button is pressed\n",
    "submit_widget = dbutils.widgets.getArgument(\"submit\")\n",
    "if submit_widget == \"Submit\":\n",
    "   about, company_size, company_type, followers, avatar, image, post_text, post_type = on_submit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5cfa544-ee61-4a23-99f1-c4e977055766",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Step 2. Run all cells from Functions & Imports including Gemini & LinkedIn Companies Model sections, until the Reddit Model section. Don't run Reddit model! Then, run the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfc9e3b5-c4ac-4c4c-b463-bc7a55211464",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/markdown": [
       "> **Title: Data Science Sorcerer Seeking a Magical Realm to Cast Spells of Innovation**\n",
       "> \n",
       "> Greetings, esteemed data wizards and tech mystics!\n",
       "> \n",
       "> As I approach the grand graduation portal, my heart flutters with anticipation as I embark on a quest for my dream data science role. With my analytical wand, I weave incantations that transform raw data into shimmering insights, unveiling the hidden magic within.\n",
       "> \n",
       "> Beyond the confines of spreadsheets, I am a conjurer of laughter and an architect of illusions. I wield humor as a potent elixir, capable of confounding even the most enigmatic data puzzles. Like a seasoned dungeon master, I excel in collaboration, quick wit, and the occasional chuckle that disrupts the monotony.\n",
       "> \n",
       "> I seek a mystical realm where innovation sparkles like a celestial river, creativity blooms in abundance, and laughter echoes through the hallways. I aspire to join a fellowship where I can cast my data science spells for the common good, while keeping the spirits buoyant and the smiles beaming.\n",
       "> \n",
       "> If you are captivated by this fusion of data science sorcerer and playful mischief-maker, let us embark on an epic adventure together! I am eager to demonstrate my prowess in transmuting data into iridescent gold, all while ensuring the energy remains electrifying and the laughter contagious.\n",
       "> \n",
       "> Join me on this mystical quest, where we shall weave a vibrant tapestry of data-driven magic and playful enchantment. #DataSorcerer #ImaginationWeaver #LaughterAlchemist\n",
       "> \n",
       "> Let's ignite our wands and illuminate the realm of data with a touch of sorcery! #QuestForDataMagic #JoinTheEnchantedAdventure"
      ],
      "text/plain": [
       "Out[42]: ",
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response, pred_score = post_success(followers,10,post_type,post_text)\n",
    "to_markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84ca7af6-d1b6-47cf-bd42-b0a6641d7e90",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Step 3. Upload the reddit dataset and paste the path to the file below. Run that cell, then run the Reddit Model section below (you can choose 'run all below' over there). Then, run the 3 following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b2ff985-7690-45d3-aa86-350440bd27a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_path = '/Workspace/Users/idanhorowitz@campus.technion.ac.il/reddit_scraped_full.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cccbc99-ab26-4b7a-90b9-bc1d74e085a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best time-frame to upload your post is: Morning\n"
     ]
    }
   ],
   "source": [
    "time_pred_df = create_time_prediction_df(response.text, 1, encode_post_type_category(post_type))\n",
    "print(f'The best time-frame to upload your post is: {pipeline_reddit.predict(time_pred_df)[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92d2cf8e-11a9-4c9c-8a47-496388c09af6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the following prompt into an image generation model, such as craiyon.com\n"
     ]
    }
   ],
   "source": [
    "if image != \"No, please give me an effecrtive prompt that I can use myself\":\n",
    "    prompt = post_type+\" \"+post_text+\" , mdjrny-v4 style\"\n",
    "    image = pipe(prompt).images[0]\n",
    "    image.save(\"./generated_image.png\")\n",
    "\n",
    "else:\n",
    "    print(\"Enter the following prompt into an image generation model, such as craiyon.com\")\n",
    "    prompt = f\"I want to create an image for a linkedin post about {post_type}. This is the full post :{response.text} Write me a prompt for an image generation model that will provide the best results\"\n",
    "    response = gemini_model.generate_content(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e6b3eec-9895-4579-ab8e-aa25e9ac1f54",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/markdown": [
       "> **Image Generation Prompt:**\n",
       "> \n",
       "> Generate an enchanting fantasy artwork depicting a data sorcerer amidst a swirling vortex of sparkling light and energy. The sorcerer, enveloped in an ethereal glow, wears a mischievous smirk and wields a radiant wand that streams luminescent data particles. The vortex symbolizes the boundless realm of digital data, its shimmering hues creating an atmosphere of enchantment and mystery. Infuse the scene with a playful, enigmatic charm, enhancing it with a hint of mischief and shimmering glitter in the air. Capture the captivating essence of the data sorcerer and the magical tapestry of digital data."
      ],
      "text/plain": [
       "Out[53]: ",
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " to_markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0f414bc-6799-410f-be60-b5df9ba1cd51",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Functions & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bdb6c96-851e-433f-b780-3f5a6ed581b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import explode, col, to_date\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.ml.feature import Word2Vec,Tokenizer,StringIndexer, OneHotEncoder,VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.regression import LinearRegression, GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.sql import Row\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Explode Updates Column\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e91fd65a-90b8-4c37-9204-805069a67ec0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c019ec5-8773-4f44-bf3d-9871b077bf01",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import textwrap\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "\n",
    "\n",
    "def to_markdown(text):\n",
    "  text = text.replace('•', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88fcabaf-b611-4f7d-933d-f4fffca077d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "genai.configure(api_key='AIzaSyDj7uxvLsUtTY6gNz8JABrDNQpxmVz-rvs')\n",
    "gemini_model = genai.GenerativeModel('gemini-pro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2f76959-c4d1-48a0-8692-339860fe1588",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### LinkedIn Companies Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f72cfe30-5973-4606-b4f4-256b630156c9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eec81ab0-f1c3-4428-8341-fd377a5240b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "companies = spark.read.parquet('/linkedin/companies')\n",
    "df = companies\n",
    "# Filter out empty lists\n",
    "df_filtered = df.filter(\"size(updates) > 0\")\n",
    "\n",
    "# Explode the 'updates' column\n",
    "df_exploded = df_filtered.selectExpr(\"*\", \"explode(updates) as exploded_updates\")\n",
    "\n",
    "# Extract values into separate columns\n",
    "df_result = df_exploded.selectExpr(\n",
    "    \"*\",\n",
    "    \"exploded_updates.comments_count as exploded_comments_count\",\n",
    "    \"exploded_updates.likes_count as exploded_likes_count\",\n",
    "    \"exploded_updates.text as exploded_text\",\n",
    "    \"exploded_updates.title as exploded_title\"\n",
    ")\n",
    "df_filtered_text = df_result.filter(df_result.exploded_text.isNotNull())\n",
    "df_filtered_text = df_filtered_text.filter(df_filtered_text.exploded_likes_count.isNotNull())\n",
    "df_filtered_text = df_filtered_text.filter(df_filtered_text.exploded_comments_count.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b1cef2e-c76e-43ed-9a35-3cf8c6e55137",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StringType, ArrayType, IntegerType\n",
    "\n",
    "# Define lists of keywords for each category\n",
    "education_keywords = [\"education\", \"learn\", \"school\", \"student\", \"classroom\", \"teach\", \"study\", \"knowledge\", \"curriculum\", \"training\", \"university\", \"college\", \"academics\", \"lecturer\", \"degree\", \"academic\", \"research\", \"scholarship\", \"student\", \"academy\"]\n",
    "inspiration_keywords = [\"inspire\", \"motivate\", \"encourage\", \"uplift\", \"inspiration\", \"empower\", \"positive\", \"success\", \"ambition\", \"hope\", \"encouragement\", \"motivation\", \"empowerment\", \"drive\", \"aspiration\", \"achievement\", \"perseverance\", \"optimism\", \"dream\"]\n",
    "science_tech_keywords = [\"science\", \"technology\", \"innovation\", \"research\", \"tech\", \"engineering\", \"computer\", \"data\", \"experiment\", \"invention\", \"discovery\", \"development\", \"informatics\", \"scientific\", \"cyber\", \"biotechnology\", \"nanotechnology\", \"artificial intelligence\", \"robotics\",\"ai\",\"machine learning\",\"ml\",\"deep learning\"]\n",
    "work_keywords = [\"work\", \"career\", \"job\", \"employment\", \"professional\", \"office\", \"business\", \"income\", \"opportunity\", \"resume\", \"#jobs\", \"hiring\", \"#hiring\", \"cv\", \"occupation\", \"job market\", \"workplace\", \"salary\", \"interview\", \"employee\",\"hr\"]\n",
    "\n",
    "\n",
    "# Define a function to count words from each category in the text\n",
    "def count_category_words(text):\n",
    "    education_count = sum(1 for word in text.lower().split() if word in education_keywords)\n",
    "    inspiration_count = sum(1 for word in text.lower().split() if word in inspiration_keywords)\n",
    "    science_tech_count = sum(1 for word in text.lower().split() if word in science_tech_keywords)\n",
    "    work_count = sum(1 for word in text.lower().split() if word in work_keywords)\n",
    "    return [education_count, inspiration_count, science_tech_count, work_count]\n",
    "\n",
    "# Create UDF to apply the counting function to the text column\n",
    "count_category_words_udf = udf(count_category_words, ArrayType(IntegerType()))\n",
    "\n",
    "# Apply the UDF to count words from each category\n",
    "df_word_counts = df_filtered_text.withColumn(\"category_counts\", count_category_words_udf(col(\"exploded_text\")))\n",
    "\n",
    "# Define a function to classify text based on word counts\n",
    "def classify_category_from_counts(counts):\n",
    "    max_count = max(counts)\n",
    "    if max_count == 0:\n",
    "        return \"General\"\n",
    "    elif max_count == counts[0]:\n",
    "        return \"Education & Self-Improvement\"\n",
    "    elif max_count == counts[1]:\n",
    "        return \"Inspiration\"\n",
    "    elif max_count == counts[2]:\n",
    "        return \"Science & Tech\"\n",
    "    else:\n",
    "        return \"Work\"\n",
    "\n",
    "# Create UDF to apply the classification function to the category counts\n",
    "classify_category_from_counts_udf = udf(classify_category_from_counts, StringType())\n",
    "\n",
    "# Apply the UDF to classify categories\n",
    "df_with_category = df_word_counts.withColumn(\"category\", classify_category_from_counts_udf(col(\"category_counts\")))\n",
    "df_with_category=df_with_category.drop(\"category_counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbf8c915-d695-4b47-b00f-3f082c404844",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_with_category_for_reg = df_with_category\n",
    "# Drop rows with missing values in relevant columns\n",
    "df_with_category_for_reg = df_with_category_for_reg.dropna(subset=[\"exploded_text\", \"exploded_likes_count\", \"category\", \"type\"])\n",
    "df_with_category_for_reg = df_with_category_for_reg.fillna({'followers': 0})\n",
    "\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer(inputCol=\"exploded_text\", outputCol=\"words\")\n",
    "df_with_category_for_reg = tokenizer.transform(df_with_category_for_reg)\n",
    "\n",
    "# Learn a Word2Vec model from the text\n",
    "word2vec = Word2Vec(vectorSize=30, minCount=5, inputCol=\"words\", outputCol=\"word2vec_features\")\n",
    "trained_w2v_post = word2vec.fit(df_with_category_for_reg)\n",
    "df_with_category_for_reg = trained_w2v_post.transform(df_with_category_for_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97f678d6-0cc7-4583-9e2a-e0429461a3bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_with_category_for_reg = df_with_category_for_reg.fillna({'about': ''})\n",
    "tokenizer2 = Tokenizer(inputCol=\"about\", outputCol=\"about_words\")\n",
    "df_with_category_for_reg = tokenizer2.transform(df_with_category_for_reg)\n",
    "word2vec2 = Word2Vec(vectorSize=30, minCount=5, inputCol=\"about_words\", outputCol=\"about_features\")\n",
    "trained_w2v_about = word2vec2.fit(df_with_category_for_reg)\n",
    "df_with_category_for_reg = trained_w2v_about.transform(df_with_category_for_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f355a61-0dea-4c2d-8ab2-306dbe64df04",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_with_category_for_reg = df_with_category_for_reg.filter(df_with_category_for_reg['exploded_likes_count'] <= 3000)\n",
    "df_with_category_for_reg = df_with_category_for_reg.withColumn('score', col('exploded_likes_count') + 2*col('exploded_comments_count'))\n",
    "df_with_category_for_reg = df_with_category_for_reg.withColumn(\"company_size\", F.regexp_extract(F.col(\"company_size\"), r'(\\d+)-?(\\d*)\\s*employees', 2)) \n",
    "df_with_category_for_reg = df_with_category_for_reg.withColumn(\"company_size\", df_with_category_for_reg[\"company_size\"].cast(\"int\"))\n",
    "df_with_category_for_reg = df_with_category_for_reg.fillna({'company_size': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c7dd745-0cb6-4fba-a3a7-f84bda9900b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_with_category_for_reg = df_with_category_for_reg.withColumn('exploded_likes_count', col('exploded_likes_count').cast(\"double\"))\n",
    "mean_value = df_with_category_for_reg.selectExpr(f'avg(exploded_likes_count)').collect()[0][0]\n",
    "stddev_value = df_with_category_for_reg.selectExpr(f'stddev(exploded_likes_count)').collect()[0][0]\n",
    "min_value = df_with_category_for_reg.selectExpr(f'min(exploded_likes_count)').collect()[0][0]\n",
    "max_value = df_with_category_for_reg.selectExpr(f'max(exploded_likes_count)').collect()[0][0]\n",
    "#scaled_target_column = ((col('exploded_likes_count') - mean_value) / stddev_value).alias('scaled_target')\n",
    "scaled_target_column = ((col('exploded_likes_count') - min_value) / (max_value - min_value)).alias('scaled_target')\n",
    "df_with_category_for_reg = df_with_category_for_reg.select('*', scaled_target_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc4498a8-fd86-4ed7-ba8b-30b9cea576d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "default_logo = \"https://static.licdn.com/aero-v1/sc/h/cs8pjfgyw96g44ln9r7tct85f\"\n",
    "df_with_category_for_reg = df_with_category_for_reg.withColumn(\"logo\", F.when(df_with_category_for_reg[\"logo\"] == default_logo, 0).otherwise(1)) \n",
    "\n",
    "# Index the \"category\" column\n",
    "indexer_category = StringIndexer(inputCol=\"category\", outputCol=\"category_index\")\n",
    "indexed_df = indexer_category.fit(df_with_category_for_reg).transform(df_with_category_for_reg)\n",
    "\n",
    "# Index the \"type\" column\n",
    "indexer_type = StringIndexer(inputCol=\"type\", outputCol=\"type_index\")\n",
    "indexed_df = indexer_type.fit(indexed_df).transform(indexed_df)\n",
    "\n",
    "# Combine features\n",
    "feature_cols = [\"category_index\", \"type_index\", 'followers', 'logo', 'about_features', 'company_size', \"word2vec_features\"]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "final_data = assembler.transform(indexed_df).select(\"features\", col('scaled_target').cast(\"double\").alias(\"label\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a528d988-4b64-449d-ace1-004f33fed73b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55e28d8b-7a5f-4791-9231-689e8527fd43",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_data, test_data = final_data.randomSplit([0.8, 0.2], seed=123)\n",
    "\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"label\")\n",
    "rf_model = rf.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d7bdda5-673a-48ba-a53e-4635a6fbe085",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### User Inputs and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff5c9dcc-5f76-48d9-81d9-13367965beb2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_data_rf (data, text_column,trained_w2v_post, trained_w2v_about):\n",
    "    # Tokenize the text\n",
    "    tokenizer = Tokenizer(inputCol=text_column, outputCol=\"words\")\n",
    "    data_for_reg = tokenizer.transform(data)\n",
    "\n",
    "    data_for_reg = trained_w2v_post.transform(data_for_reg)\n",
    "\n",
    "    data_for_reg = data_for_reg.fillna({'about': ''})\n",
    "    tokenizer2 = Tokenizer(inputCol=\"about\", outputCol=\"about_words\")\n",
    "    data_for_reg = tokenizer2.transform(data_for_reg)\n",
    "    data_for_reg = trained_w2v_about.transform(data_for_reg)\n",
    "\n",
    "    #data_for_reg = data_for_reg.withColumn(\"logo\", F.when(data_for_reg[\"logo\"] == default_logo, 0).otherwise(1)) \n",
    "\n",
    "\n",
    "    # Index the \"category\" column\n",
    "    indexer_category = StringIndexer(inputCol=\"category\", outputCol=\"category_index\")\n",
    "    indexed_df = indexer_category.fit(data_for_reg).transform(data_for_reg)\n",
    "\n",
    "    # Index the \"type\" column\n",
    "    indexer_type = StringIndexer(inputCol=\"type\", outputCol=\"type_index\")\n",
    "    indexed_df = indexer_type.fit(indexed_df).transform(indexed_df)\n",
    "\n",
    "    # Combine features\n",
    "    feature_cols = [\"category_index\", \"type_index\", 'followers', 'logo', 'about_features', 'company_size', \"word2vec_features\"]\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "    final_data = assembler.transform(indexed_df).select(\"features\")\n",
    "\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fbdadfd-0fdd-469e-8492-f62e6de9f9e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_linkedin_post_dataframe(post, post_type, category, company_size, logo, followers, about):\n",
    "    data = [\n",
    "        (post, post_type, category, company_size, logo, followers, about)\n",
    "    ]\n",
    "\n",
    "    # Define the schema\n",
    "    schema = [\"post\", \"type\", \"category\", \"company_size\", \"logo\", \"followers\", \"about\"]\n",
    "\n",
    "    # Create RDD from data\n",
    "    rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = rdd.map(lambda x: Row(**{k: v for k, v in zip(schema, x)})).toDF()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6ca5c15-855e-45d1-b7d1-90b90ddfad80",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def post_success(threshold, iters_threshold, post_type, post_text):\n",
    "    best_score = 0\n",
    "    best_response = None\n",
    "\n",
    "    current_score = 0\n",
    "    current_iter = 0\n",
    "\n",
    "    while current_score < threshold and current_iter < iters_threshold:\n",
    "        if current_iter == 0:\n",
    "            question = \"Please write me a LinkedIn post about \" + post_type + \". The post must contain a title, and the content of the post should be based on the following: \" + post_text + \". The post should be as dynamic and engaging as possible, to gain the most number of likes as possible.\"\n",
    "            response = gemini_model.generate_content(question)\n",
    "        else:\n",
    "            question = \"Please write me a LinkedIn post about \" + post_type + \". The post must contain a title, and the content of the post should be based on the following: \" + post_text + \". The post should be as dynamic and engaging as possible, to gain the most number of likes as possible. This is the last post you provided me \" + response.text + \". It received a score of \" + str(current_score) + \", please improve it.\"\n",
    "            response = gemini_model.generate_content(question)\n",
    "        \n",
    "        user_df = create_linkedin_post_dataframe(response.text, post_type, company_type, company_size, avatar, followers, about)\n",
    "        user_df_final = preprocess_data_rf(user_df, 'post', trained_w2v_post, trained_w2v_about)\n",
    "        user_predicted_score = rf_model.transform(user_df_final)\n",
    "        current_score = user_predicted_score.select('prediction').first()['prediction']\n",
    "        \n",
    "        if current_score > best_score:\n",
    "            best_score = current_score\n",
    "            best_response = response\n",
    "\n",
    "        current_iter += 1\n",
    "\n",
    "    if current_score >= threshold:\n",
    "        return response, current_score\n",
    "    else:\n",
    "        return best_response, best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f766ed1e-3422-4c47-a228-83a054565ad8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Reddit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e1cc3c0-2e5d-481e-ad60-d3ea2b04df5a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8100b455-95f5-417e-8fae-3818548ce25d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c1728f7-401c-4435-8239-a82a7dc85177",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "reddit_data= pd.read_csv(csv_path)\n",
    "reddit_data[\"created\"] = pd.to_datetime(reddit_data[\"created\"])\n",
    "reddit_data[\"hour\"] = reddit_data[\"created\"].dt.hour\n",
    "# Map the hour component to time categories\n",
    "reddit_data[\"time_category\"] = reddit_data[\"hour\"].apply(lambda hour:\n",
    "    \"Morning\" if 6 <= hour < 12\n",
    "    else \"Noon\" if 12 <= hour < 16\n",
    "    else \"Afternoon\" if 16 <= hour < 20\n",
    "    else \"Night\" if 20 <= hour < 23.59\n",
    "    else \"Late Night\"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bae1ddd-077d-42ca-b617-c08fb2920155",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_categories= reddit_data.copy()\n",
    "# Drop rows with missing values in necessary columns\n",
    "df_categories = df_categories.dropna(subset=[\"title\", \"score\", \"time_category\", \"category\"])\n",
    "\n",
    "# Convert score column to float\n",
    "df_categories[\"score\"] = df_categories[\"score\"].astype(float)\n",
    "min_value = df_categories['score'].min()\n",
    "max_value = df_categories['score'].max()\n",
    "\n",
    "df_categories['scaled_score'] = df_categories['score'].apply(lambda x: (x - min_value) / (max_value - min_value))\n",
    "\n",
    "# Split data into features and target variable\n",
    "X_reddit = df_categories[[\"title\", \"scaled_score\", \"category\"]]\n",
    "y_reddit = df_categories[\"time_category\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e46aefb-f800-4115-befb-718933832390",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "400af4b1-ef83-4bf4-bb91-fb88711537cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<command-3791678015592795>:19: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  X_train[\"category\"] = category_encoder.fit_transform(X_train[\"category\"])\n<command-3791678015592795>:20: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  X_test[\"category\"] = category_encoder.transform(X_test[\"category\"])\nOut[34]: Pipeline(steps=[('preprocessing',\n                 ColumnTransformer(transformers=[('word2vec',\n                                                  FunctionTransformer(func=<function word2vec_transform at 0x7f7f2abccd30>),\n                                                  'title'),\n                                                 ('count_vectorizer',\n                                                  CountVectorizer(),\n                                                  'title')])),\n                ('classifier', RandomForestClassifier())])"
     ]
    }
   ],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reddit, y_reddit, test_size=0.2, random_state=123)\n",
    "\n",
    "# Train Word2Vec model on the training data\n",
    "word2vec_model_reddit = Word2Vec(sentences=X_train[\"title\"], vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Function to convert title to Word2Vec vectors\n",
    "def word2vec_transform(titles):\n",
    "    return np.array([np.mean([word2vec_model_reddit.wv[word] for word in title.split() if word in word2vec_model_reddit.wv] or [np.zeros(100)], axis=0) for title in titles])\n",
    "\n",
    "# Define the preprocessing pipeline\n",
    "preprocessing_pipeline = ColumnTransformer([\n",
    "    (\"word2vec\", FunctionTransformer(word2vec_transform), \"title\"),\n",
    "    (\"count_vectorizer\", CountVectorizer(), \"title\"),\n",
    "])\n",
    "\n",
    "# Encode the category separately\n",
    "category_encoder = LabelEncoder()\n",
    "X_train[\"category\"] = category_encoder.fit_transform(X_train[\"category\"])\n",
    "X_test[\"category\"] = category_encoder.transform(X_test[\"category\"])\n",
    "\n",
    "# Combine preprocessing with classifier\n",
    "pipeline_reddit = Pipeline([\n",
    "    (\"preprocessing\", preprocessing_pipeline),\n",
    "    (\"classifier\", RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Fit the model\n",
    "pipeline_reddit.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30c60c4f-26b0-44a2-9e61-1b4ebbba2021",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Reddit Model for User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5993ca86-5838-4971-9288-379149241308",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_title(text):\n",
    "    pattern = r'Title:\\s*(.+?)(?:\\n\\n|\\Z)'\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        title = match.group(1)\n",
    "        title = title.rstrip('*')\n",
    "        if title.startswith(\"**\"):\n",
    "            title = title[2:].lstrip()\n",
    "        \n",
    "        return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c202d66d-85ff-447d-8a16-4e7e7380b615",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_time_prediction_df(post, pred_score, category):\n",
    "    title = extract_title(post)\n",
    "\n",
    "    data = [\n",
    "    (title, pred_score, category)\n",
    "    ]\n",
    "\n",
    "    # Define the schema\n",
    "    schema = [\"title\", \"scaled_score\", \"category\"]\n",
    "\n",
    "    # Create a pandas DataFrame from the data and schema\n",
    "    df = pd.DataFrame(data, columns=schema)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dcc7f4f-28ec-4730-88bf-0aa69e4eaf1d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def encode_post_type_category(post_type):\n",
    "    categories = category_encoder.classes_\n",
    "    labels = category_encoder.transform(categories)\n",
    "    encoded_label = category_encoder.transform([post_type])[0]\n",
    "\n",
    "    return int(encoded_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2855ea06-2f78-45cb-9008-6cf8cd720ddf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def predict_time_category (time_pred_df,category):\n",
    " \n",
    "\n",
    "    pipeline.predict(time_pred_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fffbb823-f9a3-4238-9930-b7284052aed7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Generate Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "224b867b-bff0-4b3d-9187-5b41c5728857",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)\n",
       "\u001B[0;32m<command-3791678015592805>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mdiffusers\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mStableDiffusionPipeline\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m      4\u001B[0m \u001B[0mmodel_id\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"prompthero/openjourney\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m      5\u001B[0m \u001B[0mpipe\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mStableDiffusionPipeline\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_pretrained\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel_id\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtorch_dtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfloat32\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py\u001B[0m in \u001B[0;36mimport_patch\u001B[0;34m(name, globals, locals, fromlist, level)\u001B[0m\n",
       "\u001B[1;32m    169\u001B[0m             \u001B[0;31m# Import the desired module. If you’re seeing this while debugging a failed import,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m    170\u001B[0m             \u001B[0;31m# look at preceding stack frames for relevant error information.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m--> 171\u001B[0;31m             \u001B[0moriginal_result\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpython_builtin_import\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mglobals\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlocals\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfromlist\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlevel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m    172\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m    173\u001B[0m             \u001B[0mis_root_import\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mthread_local\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_nest_level\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'diffusers'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)\n\u001B[0;32m<command-3791678015592805>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mdiffusers\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mStableDiffusionPipeline\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mmodel_id\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"prompthero/openjourney\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mpipe\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mStableDiffusionPipeline\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_pretrained\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel_id\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtorch_dtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfloat32\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py\u001B[0m in \u001B[0;36mimport_patch\u001B[0;34m(name, globals, locals, fromlist, level)\u001B[0m\n\u001B[1;32m    169\u001B[0m             \u001B[0;31m# Import the desired module. If you’re seeing this while debugging a failed import,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    170\u001B[0m             \u001B[0;31m# look at preceding stack frames for relevant error information.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 171\u001B[0;31m             \u001B[0moriginal_result\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpython_builtin_import\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mglobals\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlocals\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfromlist\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlevel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    172\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    173\u001B[0m             \u001B[0mis_root_import\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mthread_local\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_nest_level\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'diffusers'",
       "errorSummary": "<span class='ansi-red-fg'>ModuleNotFoundError</span>: No module named 'diffusers'",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "\n",
    "model_id = \"prompthero/openjourney\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float32)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Full Project Interface",
   "widgets": {
    "about": {
     "currentValue": "Data Science and Engineering student at the Technion, looking for a studet position. I have gained knowledge in ML and DL, including topics such as neural networks, natural language processing, and model evaluation techniques",
     "nuid": "5b757b72-fd14-4545-8d8b-374975ed356d",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Please enter the text from the about section of your LinkedIn profile:",
      "name": "about",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "avatar": {
     "currentValue": "Profile Picture",
     "nuid": "77528201-68de-4e9c-9711-825eed1f7410",
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "Profile Picture",
      "label": "Do you have a profile picture or a stock image?:",
      "name": "avatar",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "Profile Picture",
        "Stock Image"
       ]
      }
     }
    },
    "company_size": {
     "currentValue": "100",
     "nuid": "bc7ca559-daf6-4afb-a4af-9a5657b41026",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "0",
      "label": "Please enter the size of your company (number of employees):",
      "name": "company_size",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "company_type": {
     "currentValue": "Privately Held",
     "nuid": "22f5ca92-e8e7-48bf-9173-392eb241eb8e",
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "Other",
      "label": "Please select the type of company you work in:",
      "name": "company_type",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "Self-Owned",
        "Privately Held",
        "Partnership",
        "Educational",
        "Nonprofit",
        "Government Agency",
        "Public Company",
        "Self-Employed",
        "Other"
       ]
      }
     }
    },
    "followers": {
     "currentValue": "500",
     "nuid": "f79695ce-e38d-44ff-9a8d-520baee52ee4",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "0",
      "label": "Please enter the number of followers you have:",
      "name": "followers",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "image": {
     "currentValue": "No, please give me an effecrtive prompt that I can use myself",
     "nuid": "c16f6c29-714d-4d41-9a92-dc867d73bf32",
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "No, please give me an effective prompt that I can use myself",
      "label": "Please indicate your image generation preference:",
      "name": "image",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "I am willing to wait 30 minutes for a generated image",
        "No, please give me an effective prompt that I can use myself"
       ]
      }
     }
    },
    "post_text": {
     "currentValue": "'help me write a how I am look for a job as data scientist'. I am almost finished my degree, and am creative, funny, and love board games also",
     "nuid": "5e67289b-63cd-4c75-816c-1f5dbe425107",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Please enter the text of your post, or keywords:",
      "name": "post_text",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "post_type": {
     "currentValue": "Work",
     "nuid": "c16c3778-09d8-410c-a153-9c1abaa6448b",
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "Work",
      "label": "Please select the type of post you want to create:",
      "name": "post_type",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "Work",
        "Education & Self-Improvement",
        "Science & Tech",
        "Inspiration",
        "General"
       ]
      }
     }
    },
    "submit": {
     "currentValue": "Submit",
     "nuid": "39a92cbf-4fef-41a9-b2e3-2e0a5f910bf3",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "Submit",
      "label": "Type submit and hit enter please",
      "name": "submit",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
